{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tp3mkJK0vi_",
        "outputId": "ca6b1532-0a30-4d60-b44f-938e6d22d914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name !='/device:GPU:0':\n",
        "  raise SystemError(\"Gpu not\")\n",
        "print('Gpu found at {} '.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4yo68ta01lq",
        "outputId": "0d87b8bf-e776-4282-f114-3757f925572e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gpu found at /device:GPU:0 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "qimeOhPN07tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/Master/Fall 22/DS Project/Undersampled_dataset/\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eDcNs-Y0_CI",
        "outputId": "5f2d9500-a2d0-4444-8422-954afc3e0546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Master/Fall 22/DS Project/Undersampled_dataset\n",
            "'Test dataset'\t'Train dataset'   X_train.pickle   Y_train.pickle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random"
      ],
      "metadata": {
        "id": "27UTQuwsHolC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = '/content/gdrive/MyDrive/Master/Fall 22/DS Project/Undersampled_dataset/' # taining data folder\n",
        "IMG_SIZE = 224 # image size\n",
        "CATEGORIES = [\"healthy\",\"common_rust\",\"gray_leaf_spot\",\"northern_leaf_blight\"]"
      ],
      "metadata": {
        "id": "11gTHiqSHoI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "undersampled_training_data = []\n",
        "#/content/gdrive/MyDrive/Master/Fall 22/DS Project/Training /healthy\n",
        "def create_training_data():\n",
        "\n",
        "    for category in CATEGORIES: \n",
        "\n",
        "        path = os.path.join(TRAIN_DIR,category)  # create path \n",
        "        class_num = CATEGORIES.index(category)  # get the classification  \n",
        "\n",
        "        for img in tqdm(os.listdir(path)): \n",
        "          # iterate over each image per dogs and cats\n",
        "          img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_COLOR)  # convert to array\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "          undersampled_training_data.append([new_array, class_num])  # add this to our training_data\n",
        "            \n",
        "    random.shuffle(undersampled_training_data)\n",
        "    #np.save('train_data1.npy', training_data)        \n",
        "\n",
        "create_training_data()\n",
        "#np.save('train_data.npy', training_data)   \n",
        "print(\"Train data size: \",len(undersampled_training_data))\n",
        "\n",
        "#print(\"train\")\n",
        "print()\n",
        "X_train = np.array([i[0] for i in undersampled_training_data]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
        "Y_train = [i[1] for i in undersampled_training_data]\n",
        "\n",
        "pickle_out = open(\"X_train.pickle\",\"wb\")\n",
        "pickle.dump(X_train, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"Y_train.pickle\",\"wb\")\n",
        "pickle.dump(Y_train, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "\n",
        "print(\"Pickle saved.\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWXL4V08HoGy",
        "outputId": "f50ff42d-1ab6-4965-c31b-8a76457ae8f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 410/410 [00:01<00:00, 291.10it/s]\n",
            "100%|██████████| 410/410 [00:01<00:00, 303.61it/s]\n",
            "100%|██████████| 410/410 [00:01<00:00, 276.05it/s]\n",
            "100%|██████████| 410/410 [00:01<00:00, 277.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data size:  1640\n",
            "\n",
            "Pickle saved.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00qAWwgxJ2h-",
        "outputId": "59ae2a00-3a56-419d-dba3-f5445a3276bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1640, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BM5gszcpJ2fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_DIR = '/content/gdrive/MyDrive/Master/Fall 22/DS Project/Undersampled_dataset/Test dataset/' # test data folder\n",
        "IMG_SIZE = 224 # image size\n",
        "CATEGORIES = [\"healthy\",\"common_rust\",\"gray_leaf_spot\",\"northern_leaf_blight\"]"
      ],
      "metadata": {
        "id": "9dRCgmChJ2dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = []\n",
        "\n",
        "def create_test_data():\n",
        "\n",
        "    for category in CATEGORIES: \n",
        "\n",
        "        path = os.path.join(TEST_DIR,category)  # create path \n",
        "        class_num = CATEGORIES.index(category)  # get the classification  \n",
        "\n",
        "        for img in tqdm(os.listdir(path)): \n",
        "          # iterate over each image per dogs and cats\n",
        "          img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_COLOR)  # convert to array\n",
        "          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  # resize to normalize data size\n",
        "          test_data.append([new_array, class_num])  # add this to our training_data\n",
        "            \n",
        "    random.shuffle(test_data)\n",
        "    #np.save('train_data1.npy', training_data)        \n",
        "\n",
        "create_test_data()\n",
        "#np.save('train_data.npy', training_data)   \n",
        "print(\"Test data size: \",len(test_data))\n",
        "\n",
        "#print(\"Test npm created.\")\n",
        "print()\n",
        "X_test = np.array([i[0] for i in test_data]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
        "Y_test = [i[1] for i in test_data]\n",
        "\n",
        "pickle_out = open(\"X_test.pickle\",\"wb\")\n",
        "pickle.dump(X_test, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"Y_test.pickle\",\"wb\")\n",
        "pickle.dump(Y_test, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "print(\"Pickle saved.\")\n",
        "print()"
      ],
      "metadata": {
        "id": "T0hswr0PHn_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473b8e6e-0109-41ce-ceb7-b10a071d0130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 234/234 [00:04<00:00, 52.38it/s] \n",
            "100%|██████████| 238/238 [00:03<00:00, 59.58it/s] \n",
            "100%|██████████| 103/103 [00:02<00:00, 38.83it/s]\n",
            "100%|██████████| 197/197 [00:03<00:00, 57.79it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data size:  772\n",
            "\n",
            "Pickle saved.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}